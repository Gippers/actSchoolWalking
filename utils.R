###########################
# A file for storing general helper function.
# Author: Thomas Martin/GPT-5/Cluade 4 Sonnet
# Date: 30/08/2025
############################

# AI Generated - GPT-5 (Human Checked and Edited)
library(data.table)
library(sf)

clean_act_erp <- function(file_path) 
{
  #' @description Load the ERP data from 2001 to 2021 and clean it to get only 
  #' required information. Then, turn it into and sf datatype by adding  
  #' polygons to the dataset.  Then saves the data locally
  #' 

  ###############################
  # Load ERP Data
  ########################
  # Load only needed columns to save memory
  cols_to_keep <- c("Sex", "Age", "ASGS_2021", "Region", "TIME_PERIOD", "OBS_VALUE")
  
  dt <- fread(
    file_path,
    select = cols_to_keep,
    showProgress = TRUE
  )
  
  # Filter ACT SA2s (codes starting with "8") and length 9
  # as the datset contains shorte
  dt <- dt[grepl("^8", ASGS_2021) &
          nchar(ASGS_2021) == 9]

  # Filter out some SA2's that either have small population or are not 
  # in resonable walking distance (such as south canberra). Note
  # some with 0 pop will be kept as this dataset will be used to filter
  # the path network, and deleting them may break the path network
  name.drop <- c("ACT - South West", "Namadgi", "Tuggeranong - West", 
                  "Tuggeranong", "Hume", "Kowen", "Majura", 
                "Canberra Airport", "Gungahlin - East")
  dt <- dt[!(Region %in% name.drop)]

  # Keep only "Persons" (total population)
  dt <- dt[Sex == "Persons"]
  dt[, Sex := NULL]
  
  # Keep only 3 age groups relating to primary and highschool aged students
  dt <- dt[Age %in% c("5-9", "10-14", "15-19")]
  
  # Rename columns for clarity
  setnames(dt, c("ASGS_2021", "Region", "TIME_PERIOD", "Age", "OBS_VALUE"),
               c("sa2_code", "sa2_name", "year", "age", "erp"))
  

  ####################################
  # Load the sa2 shapefile
  sa2_shp <- list.files("data_large", pattern = "(?i)SA2.*2021.*\\.shp$", 
                        recursive = TRUE, full.names = TRUE)
  stopifnot(length(sa2_shp) >= 1)
  sa2 <- st_read(sa2_shp[1], quiet = TRUE)

  # Identify column names (ABS bundles vary slightly)
  nm <- names(sa2)
  ste_col <- if ("STE_NAME21" %in% nm) "STE_NAME21" else "STE_NAME_2021"
  sa2_col <- if ("SA2_NAME21" %in% nm) "SA2_NAME21" else "SA2_NAME_2021"

  # Filter to ACT & Braddon SA2
  sa2_act <- sa2 |> filter(.data[[ste_col]] == "Australian Capital Territory")

  # Ensure WGS84 for OSM/dodgr
  sa2_act <- st_transform(sa2_act, 4326)
    
  ##################################
  # Combine the shapefile and data.table


  # Remove uncessary columns in sa2_act
  sa2_act <- sa2_act |> 
    select(SA2_CODE21, geometry)

  # Join dt with sa2_act using SA2 codes
  # Note: sa2_act has SA2_CODE21, dt has sa2_code
  dt_sf <- dt |>
    left_join(sa2_act, by = c("sa2_code" = "SA2_CODE21")) 

  dt_sf <- st_as_sf(dt_sf)

  # Check the result
  cat("Original dt rows:", nrow(dt), "\n")
  cat("SA2 ACT polygons:", nrow(sa2_act), "\n") 
  cat("Joined sf rows:", nrow(dt_sf), "\n")
  cat("Is polygon empty?:", sum(st_is_empty(dt_sf)), "\n") # Check polygon is empty

  # Save dataset
  st_write(dt_sf, "data/erp_sa2.gpkg", delete_dsn = TRUE)
}


# Generated by Claude 3.5 Sonnet
# Updated function to clean and process school location data for ACT analysis
clean_school_locations <- function(file_path) {
  #' @description Load school location data from ODS file, clean coordinates,
  #' and convert to sf format for spatial analysis in the ACT
  #' @param file_path (character): Path to the school_location.ods file
  
  library(readODS)
  library(sf)
  library(tidyr)
  library(stringr)

  
  # Read the ODS file from the 2001_geo sheet
  schools_raw <- read_ods(file_path, sheet = "2001_geo")
  
  # Clean the coordinate data
  schools_clean <- schools_raw |>
    # Remove rows with missing coordinates
    filter(!is.na(Latlong) & Latlong != "") |>
    mutate(
      # Clean coordinate string - remove parentheses and extra spaces
      coords_clean = str_replace_all(Latlong, "[\\(\\)]", ""),
      coords_clean = str_trim(coords_clean),
      # Remove trailing commas
      coords_clean = str_replace(coords_clean, ",$", "")
    ) |>
    # Separate coordinates into lat and lon
    separate(coords_clean, into = c("latitude", "longitude"), sep = ",") |>
    mutate(
      # Convert to numeric and handle missing values
      latitude = as.numeric(str_trim(latitude)),
      longitude = as.numeric(str_trim(longitude)),
      # Fix positive latitudes (should be negative for Australia)
      latitude = ifelse(latitude > 0, -latitude, latitude)
    ) |>
    # Remove rows where coordinate conversion failed
    filter(!is.na(latitude) & !is.na(longitude)) |>
    # Clean other columns
    mutate(
      school = str_trim(school),
      type = str_trim(type),
      open = as.numeric(open),
      close = as.numeric(close),
      Notes = str_trim(Notes)
    ) |>
    # Remove the original Latlong column
    select(-Latlong)
  
  # Convert to sf object
  schools_sf <- schools_clean |>
    st_as_sf(coords = c("longitude", "latitude"), crs = 4326)
  
  # Add back coordinate columns using separate extraction
  coords <- st_coordinates(schools_sf)
  schools_sf <- schools_sf |>
    mutate(
      longitude = coords[,1],
      latitude = coords[,2]
    )
  
  
  
  # Create data/cleaned directory if it doesn't exist
  if (!dir.exists("data/cleaned")) {
    dir.create("data/cleaned", recursive = TRUE)
  }
  
  # Save the cleaned dataset
  st_write(schools_sf, "data/cleaned/school_locations.gpkg", delete_dsn = TRUE)
  
  cat("Processed", nrow(schools_sf), "school locations\n")
  cat("Schools by type:\n")
  print(table(schools_sf$type))
  cat("Date range:", min(schools_sf$open, na.rm = TRUE), "to", 
      max(schools_sf$close, na.rm = TRUE), "\n")
  cat("Saved to: data/cleaned/school_locations.gpkg\n")
  
  return(schools_sf)
}

# Usage example:
# schools_sf <- clean_school_locations("data/school_location.ods")